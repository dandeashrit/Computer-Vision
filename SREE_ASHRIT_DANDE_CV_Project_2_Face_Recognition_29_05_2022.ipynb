Mount google drive into sample colab space
from google.colab import drive
drive.mount('/content/drive')
!ls
• DOMAIN: Entertainment

• CONTEXT: Company X owns a movie application and repository which caters movie streaming to millions of users who on subscription basis.
Company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on the movie
and clicks on cast information button, the app will show details of the actor in the scene. Company has an in-house computer vision and
multimedia experts who need to detect faces from screen shots from the movie scene. The data labelling is already done. Since there higher time complexity is involved in the

• DATA DESCRIPTION: The dataset comprises of images and its mask for corresponding human face.

• PROJECT OBJECTIVE: To build a face detection system.
Steps and tasks: [ Total Score: 20 Marks]
# Import all the necessary libraries.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import cv2
from google.colab.patches import cv2_imshow
import os

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split

import tensorflow as tf 
%tensorflow_version 2.x
import tensorflow
tensorflow.__version__
1. Import and Understand the data [7 Marks]

A. Import and read ‘images.npy’. [1 Marks]
images_data = np.load('/content/drive/MyDrive/CV_Project_2/images.npy', allow_pickle=True)
images_data.shape
B. Split the data into Features(X) & labels(Y). Unify shape of all the images. [3 Marks]

Imp Note: Replace all the pixels within masked area with 1.
Hint: X will comprise of array of image whereas Y will comprise of coordinates of the mask(human face). Observe: data[0], data[0][0], data[0][1].
features = images_data[:,0]
labels = images_data[:,1]
features.shape,labels.shape
images_data[0]
images_data[0][0]
images_data[0][1]
# Unifying the image height and width to (@56,256)
img_ht = 256
img_wh = 256
# Store label and features after unifying the shape as an empty array.

label_fin = np.zeros((int(images_data.shape[0]), img_ht, img_wh))
img_fin = np.zeros((int(images_data.shape[0]), img_ht, img_wh, 3))
label_fin.shape, img_fin.shape
for ind in range(images_data.shape[0]):
    image = images_data[ind][0]
    image = cv2.resize(image, dsize=(img_ht, img_wh), interpolation=cv2.INTER_CUBIC)
    try:
      image = image[:, :, :3]
    except:
      continue
    img_fin[ind] = tensorflow.keras.applications.mobilenet_v2.preprocess_input(np.array(image, dtype=np.float32))
    for i in images_data[ind][1]:
        x1 = int(i["points"][0]['x'] * img_wh)
        x2 = int(i["points"][1]['x'] * img_wh)
        y1 = int(i["points"][0]['y'] * img_ht)
        y2 = int(i["points"][1]['y'] * img_ht)
        label_fin[ind][y1:y2, x1:x2] = 1
print("The unified shape of image features is :" , img_fin.shape)
print("The unified shape of labels mask is :", label_fin.shape)
C. Split the data into train and test[400:9]. [1 Marks]
# Split train and test data in [400:9]
X_train, X_test, y_train, y_test = train_test_split(img_fin, label_fin, test_size=9, random_state=20)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
D. Select random image from the train data and display original image and masked image. [2 Marks]
# Display original training image which is at index 2
fig = plt.figure(figsize=(20, 20))
plt.subplot(1,5,1)
plt.imshow(X_train[2])  
plt.tight_layout() 
plt.show()
# Display corresponding Mask for training image which is at index 2
fig = plt.figure(figsize=(20, 20))
plt.subplot(1,5,1)
plt.imshow(y_train[2]) 
plt.tight_layout()
plt.show()
2. Model building [11 Marks]
from numpy.core.fromnumeric import shape
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Conv2D,Activation, BatchNormalization,Concatenate, Input, Reshape, UpSampling2D
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.metrics import Recall, Precision
from tensorflow.keras.models import Model
  A. Design a face mask detection model. [4 Marks]

  Hint: 1. Use MobileNet architecture for initial pre-trained non-trainable layers.
  
  Hint: 2. Add appropriate Upsampling layers to imitate U-net architecture.
# MobileNet Arcgitecture to train input layer using pre-trained imagenet weights

input =Input(shape= (img_ht, img_wh, 3),name= 'img_input')
encoder = MobileNetV2(input_tensor=input, include_top=False, alpha=0.35, weights="imagenet")
encoder.summary()
def create_model():

  input = Input(shape=(img_ht, img_wh, 3),name= "img_input")
  #Pre-trained Encoder
  encoder = MobileNetV2(input_tensor= input, include_top=False, alpha= 0.35, weights="imagenet")
  skip_connection_list = ["img_input", "block_1_expand_relu", "block_3_expand_relu", "block_6_expand_relu"]
  enc_output =encoder.get_layer("block_13_expand_relu").output
 
  f = [16, 32, 48, 64]
  X = enc_output
  for i in range(1,len(skip_connection_list)+1,1):
    X_skip = encoder.get_layer(skip_connection_list[-i]).output
    print(X_skip)
    X = UpSampling2D([2,2])(X)
    X = Concatenate()([X, X_skip])

    X = Conv2D(f[-i], (3, 3), padding="same")(X)
    X = BatchNormalization()(X)
    X = Activation("relu")(X)
        
    X = Conv2D(f[-i], (3, 3), padding="same")(X)
    X = BatchNormalization()(X)
    X = Activation("relu")(X)
        
  X = Conv2D(1, (1, 1), padding="same")(X)
  X = Activation("sigmoid")(X)

  model = Model(input,X)
  return model
model = create_model()
model.summary()
B. Design your own Dice Coefficient and Loss function. [2 Marks]
def dice_coefficient(y_true, y_pred):
    num = 2 * tensorflow.reduce_sum(y_true * y_pred)
    den = tensorflow.reduce_sum(y_true + y_pred)

    return num/ (den + tensorflow.keras.backend.epsilon())
def dice_loss(y_true, y_pred):
    return binary_crossentropy(y_true, y_pred) - tensorflow.keras.backend.log(dice_coefficient(y_true, y_pred) + tensorflow.keras.backend.epsilon())
C. Train and tune the model as required. [3 Marks]
optimizer = tensorflow.keras.optimizers.Adam(1e-4)
metric = [dice_coefficient, Recall(), Precision()]
model.compile(loss = dice_loss, optimizer = optimizer, metrics = metric)
model.fit( X_train,y_train, epochs = 25, batch_size = 10, steps_per_epoch = 40) 
test_steps = len(X_test)
model.evaluate(X_test, y_test, steps = test_steps)
3. Test the model predictions on the test image: ‘image with index 3 in the test data’ and visualise the predicted masks on the faces in the image. [2 Marks]
X_test[3]
test_img = X_test[3]
y_pred = model.predict(np.array([test_img]))
y_pred
pred_mask = cv2.resize((1.0*(y_pred[0] > 0.5)), (img_wh,img_ht))
**Viewing the predicted image and its face-detected output**
plt.imshow(test_img)
plt.imshow(pred_mask)
PART B - 10 Marks
• DOMAIN: Entertainment

• CONTEXT: Company X owns a movie application and repository which caters movie streaming to millions of users who on subscription
basis. Company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on
the movie and clicks on cast information button, the app will show details of the actor in the scene. Company has an in-house computer vision and multimedia experts who need to detect faces from screen shots from the movie scene. The data labelling is already done. Since there higher time complexity is involved in the

• DATA DESCRIPTION: The dataset comprises of face images.

• PROJECT OBJECTIVE: To create an image dataset to be used by AI team build an image classifier data. Profile images of people are given.

Steps and tasks: [ Total Score: 10 Marks]
1. Read/import images from folder ‘training_images’. [2 Marks]
from tqdm.notebook import trange, tqdm
from IPython.display import Image, display, clear_output, Markdown
import cv2
import glob
import os
import tensorflow as tf 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from zipfile import ZipFile

import warnings
warnings.filterwarnings('ignore')
project_path = '/content/drive/MyDrive/CV_Project_2/'
image_file = 'training_images-20211126T092819Z-001.zip'
# Unzip the trainin_images zip file
img_zip_loc = os.path.join(project_path, image_file)

with ZipFile(img_zip_loc, 'r') as unzip:
  unzip.extractall()
## Extract unzipped Folder Location 

zip_dir_path = unzip.filelist[0].filename.split("/")[0] 
zip_dir_path
# Reading the file names
raw_img_filename = [os.path.join(zip_dir_path,i) for i in os.listdir(zip_dir_path)]
len(raw_img_filename)
raw_img_filename[:]
img_list = []
for images in tqdm(raw_img_filename):
  img_tst = cv2.imread(images)
  img_list.append(img_tst)
img_list = np.array(img_list)
img_list.shape
# Displaying the top 15 images after resizing the images to (224,224)
from google.colab.patches import cv2_imshow

for i in img_list[:15,]:
  cv2_imshow(cv2.resize(i,(224,224)))  
2. Write a loop which will iterate through all the images in the ‘training_images’ folder and detect the faces present on all the images. [3 Marks]

Hint: You can use ’haarcascade_frontalface_default.xml’ from internet to detect faces which is available open source.
# Downloading the HAAR CASCADE FRONTAL FACE. XML FILE
!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
undetected_imgs = []
detected_imgs = []

for imgs, fnames in tqdm(zip(img_list,raw_img_filename)):
  gray = cv2.cvtColor(imgs,cv2.COLOR_BGR2GRAY)
  faces = face_cascade.detectMultiScale(gray,1.1,4)
  if len(faces) == 0:
    undetected_imgs.append(fnames)
  else:
    detected_imgs.append(fnames)

len(detected_imgs), len(undetected_imgs)
3. From the same loop above, extract metadata of the faces and write into a DataFrame. [3 Marks]
output_img_df = pd.DataFrame(columns=['x','y','w','h','Total_Faces','Image_Name'])
output_img_df
def test_bb (df,fname,title=""):

  img_tst = cv2.imread(fname)
  temp_df = df[df['Image_Name'] == fname]
  rect_img = []
  for rows in temp_df.index:
    X = df['x'][rows]
    Y = df['y'][rows]
    W = df['w'][rows]
    H = df['h'][rows]
    cv2.rectangle(img_tst, (X, Y), ( X+W, Y+H) ,(255,0,0),2)
    cv2.putText(img_tst, title, (int((X+W)*0.75), Y-3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255))
  cv2_imshow(img_tst)
  
  return
def show_face(img_list,scale=1.0):

  for images in img_list:
    image = cv2.imread(images)
    image_w  = int(image.shape[1]*scale)
    image_h = int(image.shape[0]*scale)
    image = cv2.resize(image,(image_w,image_h))
    display(Markdown(f"#### {images}"))
    cv2_imshow(image)

  return
undetected_imgs = []
detected_imgs = []

for imgs, fnames in tqdm(zip(img_list,raw_img_filename)):
  gray = cv2.cvtColor(imgs,cv2.COLOR_BGR2GRAY)
  faces = face_cascade.detectMultiScale(gray,1.1,4)
  if len(faces) == 0:
    undetected_imgs.append(fnames)
    temp_dict = {'x':0, 'y':0, 'w':-1, 'h':-1, 'Total_Faces':0, 'Image_Name':fnames} 

  else:
    detected_imgs.append(fnames)
    for (X,Y,W,H) in faces:
      temp_dict = {'x':X,'y':Y,'w': W,'h':H, 'Total_Faces':len(faces), 'Image_Name':fnames} 
      
      output_img_df = output_img_df.append(temp_dict,ignore_index=True)
output_img_df
4. Save the output Dataframe in .csv format. [2 Marks]
output_img_df.to_csv(r'/content/drive/MyDrive/CV_Project_2/output.csv', index=False)
Part C - 30 Marks
• DOMAIN: Face Recognition

• CONTEXT: Company X intends to build a face identification model to recognise human faces.

• DATA DESCRIPTION: The dataset comprises of images and its mask where there is a human face.

• PROJECT OBJECTIVE: Face Aligned Face Dataset from Pinterest. This dataset contains 10,770 images for 100 people. All images are taken
from 'Pinterest' and aligned using dlib library. Some data samples:
Steps and tasks: [ Total Score: 30 Marks]
import numpy as np
import os
import cv2
import glob
import tensorflow as tf 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from zipfile import ZipFile

import warnings
warnings.filterwarnings('ignore')

from tensorflow.keras.applications import MobileNetV2
1. Unzip, read and Load data(‘PINS.zip’) into session. [2 Marks]
path = '/content/drive/MyDrive/CV_Project_2/PINS.zip'
from zipfile import ZipFile
with ZipFile (path,'r') as pins:
  pins.extractall() 
2. Write function to create metadata of the image. [4 Marks]

Hint: Metadata means derived information from the available data which can be useful for particular problem statement.
class IdentityMetadata():
    def __init__(self, base, name, file):
        # print(base, name, file)
        # dataset base directory
        self.base = base
        # identity name
        self.name = name
        # image file name
        self.file = file

    def __repr__(self):
        return self.image_path()

    def image_path(self):
        return os.path.join(self.base, self.name, self.file) 

3. Write a loop to iterate through each and every image and create metadata for all the images. [4 Marks]
# Function to load data
def load_metadata(path):
    metadata = []
    for i in os.listdir(path):
        for f in os.listdir(os.path.join(path, i)):
            # Check file extension. Allow only jpg/jpeg' files.
            ext = os.path.splitext(f)[1]
            if ext == '.jpg' or ext == '.jpeg':
                metadata.append(IdentityMetadata(path, i, f))
    return np.array(metadata)
# Call load_metadata to read the images and assign metadata to a variable
metadata = load_metadata('PINS')
# Function to read image using opencv and convert it from BGR to RGB

def load_image(path):
    img = cv2.imread(path, 1)
    # OpenCV loads images with color channels
    # in BGR order. So we need to reverse them
    
    return img[...,::-1]
load_image(metadata[0].image_path())
4. Generate Embeddings vectors on the each face in the dataset. [4 Marks]

Hint: Use ‘vgg_face_weights.h5’
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import Model
from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation

def vgg_face():
    model = Sequential()
    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))
    model.add(Convolution2D(64, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, (3, 3), activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    model.add(Convolution2D(4096, (7, 7), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Convolution2D(4096, (1, 1), activation='relu'))
    model.add(Dropout(0.5))
    model.add(Convolution2D(2622, (1, 1)))
    model.add(Flatten())
    model.add(Activation('softmax'))
    return model
# Load the VGGFace Model
model = vgg_face()
pre_train_weights = '/content/drive/MyDrive/CV_Project_2/vgg_face_weights.h5'
# VGG Face Descriptor
vgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)
vgg_face_descriptor.inputs, vgg_face_descriptor.outputs
type(vgg_face_descriptor)
# Get embedding vector for first image in the metadata using the pre-trained model

img_path = metadata[0].image_path()
img = load_image(img_path)

# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]
img = (img / 255.).astype(np.float32)

img = cv2.resize(img, dsize = (224,224))
print(img.shape)

# Obtain embedding vector for an image
# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape 

embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]
print(embedding_vector.shape)
total_images = len(metadata)
print(total_images)
embeddings = np.zeros((metadata.shape[0], 2622))

for i, m in enumerate(metadata):
    img_path = metadata[i].image_path()
    img = load_image(img_path)
    img = (img / 255.).astype(np.float32)
    img = cv2.resize(img, dsize = (224,224))
    embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]
    embeddings[i]=embedding_vector
embeddings.shape
5. Build distance metrics for identifying the distance between two similar and dissimilar images. [4 Marks]
### Function to calculate distance between given 2 pairs of images.

- Consider distance metric as "Squared L2 distance"
- Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2
def distance(emb1, emb2):
    return np.sum(np.square(emb1 - emb2))
#### Plot images and get distance between the pairs given below
- 2, 3 and 2, 180
- 30, 31 and 30, 100
- 70, 72 and 70, 115
import matplotlib.pyplot as plt

def show_pair(idx1, idx2):
    plt.figure(figsize=(8,3))
    plt.suptitle(f'Distance between {idx1} & {idx2}= {distance(embeddings[idx1], embeddings[idx2]):}')
    plt.subplot(121)
    plt.imshow(load_image(metadata[idx1].image_path()))
    plt.subplot(122)
    plt.imshow(load_image(metadata[idx2].image_path()));    

show_pair(2, 3)
show_pair(2, 180)
show_pair(30, 31)
show_pair(30, 100)
show_pair(70, 72)
show_pair(70, 115)
6. Use PCA for dimensionality reduction. [2 Marks]
- Create X_train, X_test and y_train, y_test
- Use train_idx to seperate out training features and labels
- Use test_idx to seperate out testing features and labels
train_idx = np.arange(metadata.shape[0]) % 9 != 0
test_idx = np.arange(metadata.shape[0]) % 9 == 0

# one half as train examples of 10 identities
X_train = embeddings[train_idx]
# another half as test examples of 10 identities
X_test = embeddings[test_idx]

targets = np.array([m.name for m in metadata])
y_train = targets[train_idx] #train labels
y_test = targets[test_idx]  #test labels
display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
- Encode the targets
- Use LabelEncoder
from sklearn.preprocessing import LabelEncoder

LE = LabelEncoder()
y_train_en = LE.fit_transform(y_train)
y_test_en = LE.transform(y_test)

- Scale the features using StandardScaler
# Standarize features
from sklearn.preprocessing import StandardScaler
# Standarize features
SC = StandardScaler()
X_train_sc = SC.fit_transform(X_train)
X_test_sc = SC.transform(X_test)
- Reduce feature dimensions using Principal Component Analysis
- Set the parameter n_components=128
from sklearn.decomposition import PCA

pca = PCA(n_components=128)
X_train_pca = pca.fit_transform(X_train_sc)
X_test_pca = pca.transform(X_test_sc)
#### Add your code here ####
7. Build an SVM classifier in order to map each image to its right person. [4 Marks]
from sklearn.svm import SVC

clf = SVC(C= 10, gamma=0.01)
clf.fit(X_train_pca, y_train_en)
y_predict = clf.predict(X_test_pca)
y_predict_en = LE.inverse_transform(y_predict)
print('y_predict : ',y_predict)
print('y_test_encoded : ',y_test_en)
8. Import and display the the test images. [2 Marks]


Test_image_1=load_image('/content/drive/MyDrive/CV_Project_2/Benedict Cumberbatch9.jpg')
Test_image_2=load_image('/content/drive/MyDrive/CV_Project_2/Dwayne Johnson4.jpg')
Test_image_1 = (Test_image_1 / 255.).astype(np.float32)
Test_image_1 = cv2.resize(Test_image_1, dsize = (224,224))
Test_image_2 = (Test_image_2 / 255.).astype(np.float32)
Test_image_2 = cv2.resize(Test_image_2, dsize = (224,224))
Test_image_1.shape, Test_image_2.shape
9. Use the trained SVM model to predict the face on both test images. [4 Marks]
embedding_1 = np.zeros((1, 2622))
embedding_2 = np.zeros((1, 2622))
embedding_vector_1 = vgg_face_descriptor.predict(np.expand_dims(Test_image_1, axis=0))[0]
embedding_1[0]= embedding_vector_1
embedding_vector_2 = vgg_face_descriptor.predict(np.expand_dims(Test_image_2, axis=0))[0]
embedding_2[0]= embedding_vector_2
print(embedding_vector_1.shape)
print(embedding_vector_2.shape)
first_image = embedding_1
second_image = embedding_2
X_test_1 = SC.transform(first_image)
X_test_2 = SC.transform(second_image)
X_test_pca_1 = pca.transform(X_test_1)
X_test_pca_2 = pca.transform(X_test_2)
X_test_pca_1.shape
X_test_pca_2.shape
y_predict_1 = clf.predict(X_test_pca_1)
y_predict_enc_1 = LE.inverse_transform(y_predict_1)
plt.imshow(Test_image_1)
plt.title(f'Predicted image is {y_predict_enc_1}');
y_predict_2 = clf.predict(X_test_pca_2)
y_predict_enc_2 = LE.inverse_transform(y_predict_2)

plt.imshow(Test_image_2)
plt.title(f'Predicted image is {y_predict_enc_2}');
